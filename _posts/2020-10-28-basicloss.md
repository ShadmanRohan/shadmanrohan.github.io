---
layout: post
title: Some fundamentals behind forming the Loss Function
subtitle: MSE, MLE, Cross-Entropy
tags: [DeepLearning]
---


## Is MLE same as MSE ar Cross-Entropy

## When to use MSE and Entropy?

## Binary Cross Entropy(BCE) vs Multinomial Cross Entropy(MCE)

## Calculating loss against OH vectors vs Probability distribution

## KL Divergence is actually the negative log of corresponding element of OH+

> [useful link](https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/#:~:text=The%20difference%20between%20MLE%20and,that%20people%20typically%20care%20about.)
