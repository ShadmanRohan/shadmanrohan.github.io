---
layout: post
title: Some fundamentals behind forming the Loss Function
subtitle: MSE, MLE, Cross-Entropy
tags: [DeepLearning]
---


## Is MLE same as MSE ar Cross-Entropy

## When to use MSE and Entropy?

## Binary Cross Entropy(BCE) vs Multinomial Cross Entropy(MCE)  
https://drive.google.com/file/d/1dN55KCCuB1MWsLBSqdX31cRjWojMpqsG/view

## Calculating loss against OH vectors vs Probability distribution

## KL Divergence is the negative log of argmax(y-pred)  
https://drive.google.com/file/d/1dN55KCCuB1MWsLBSqdX31cRjWojMpqsG/view

> [useful link](https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/#:~:text=The%20difference%20between%20MLE%20and,that%20people%20typically%20care%20about.)
