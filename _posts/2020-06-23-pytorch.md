---
layout: post
title: PyTorch Notes
tags: [framework, deeplearning]
---

#### Torch Tensors

Torch tensors are the same as numpy's nd-array in terms of functionality. Numpy nd-arrays' popularity comes from it being faster than 
python's lists because it is actually [implemented in C](https://github.com/numpy/numpy/tree/master/numpy/core/src) wrapped in python. Torch tensors are originally built using C++ with support for parallel computation leveraging muliple cores in GPU.

#### Datasets and Dataloaders

Pytorch comes with built-in libraries that makes creating [custom datasets and dataloaders](https://github.com/utkuozbulak/pytorch-custom-dataset-examples) very easy.

#### common neural net mistakes:
1) you didn't try to overfit a single batch first. 
2) you forgot to toggle train/eval mode for the net. 
3) you forgot to .zero_grad() (in pytorch) before .backward(). 
4) you passed softmaxed outputs to a loss that expects raw logits.
5) you didn't use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer.
6) thinking view() and permute() are the same thing (& incorrectly using view)
7) forgetting to specify the dim/axis when calling sum, avg or max. fails silently e.g. when you're averaging batch errors for your loss function :/
8) Not shuffling training data, or otherwise using batches that have too much correlation between the examples in each batch.
9) you forgot that pytorch's .view() function reads from the last dimension first and fills the last dimension first too and are sending wrong input to model but aren't getting an error since the shape is right.
10) softmax or other loss operation over wrong dim
11) you left a relu activation before a softmax
12) using softmax instead of sigmoid in multilabel classification
13) Not shuffling training data, or otherwise using batches that have too much correlation between the examples in each batch.
14) You initialize parameters to zero as opposed to a truncated normal or xavier.

i) Using BatchNorm in a place other than before an activation function, 
ii) using softmax with a negative log-likelihood loss instead of logged softmax 
iii) Not shuffling the data every epoch 
iv) Not masking your variable length sequences

In using LSTMs: 
1) not setting shuffle to false after each epoch of training, 
2) not making the network stateful across batches, only resetting the state after each epoch, 
3) not checking if increasing the regularization strength increases the loss as well
4) not checking the activation/gradient distributions per layer to ensure a uniform distribution of activations in the applicable range of activation function, 
5) ensuring final learning rate is not at the edge of an interval in which case might be missing optimal hyoerparaneters
6) Not performing gradient clipping / normalization on LSTM networks
